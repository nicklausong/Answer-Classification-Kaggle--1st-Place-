{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, our objective is to find the optimal ngram and min_df range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import hyperopt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix, hstack, vstack\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Read in the preprocessed files\n",
    "train = pd.read_csv(\"../datasets/preprocessed_train.csv\")\n",
    "test = pd.read_csv(\"../datasets/preprocessed_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Id</th>\n",
       "      <th>num_numbers</th>\n",
       "      <th>prop_numbers</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_punctuation</th>\n",
       "      <th>prop_punctuation</th>\n",
       "      <th>nchar</th>\n",
       "      <th>word_density</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>combining lindelof's and gregg lind's ideas: l...</td>\n",
       "      <td>1</td>\n",
       "      <td>15086</td>\n",
       "      <td>3</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>80</td>\n",
       "      <td>51</td>\n",
       "      <td>0.117241</td>\n",
       "      <td>435</td>\n",
       "      <td>5.370370</td>\n",
       "      <td>[('combining', 'VBG'), ('lindelof', 'NN'), (\"'...</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.00076923 0.00076923 0.00076923 0.00076923 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in most cases r is an interpreted language tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>41061</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>234</td>\n",
       "      <td>5.850000</td>\n",
       "      <td>[('in', 'IN'), ('most', 'JJS'), ('cases', 'NNS...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.00131579 0.00131579 0.00131579 0.00131579 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i don't know r at all, but a bit of creative g...</td>\n",
       "      <td>1</td>\n",
       "      <td>34417</td>\n",
       "      <td>12</td>\n",
       "      <td>0.013423</td>\n",
       "      <td>164</td>\n",
       "      <td>49</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>894</td>\n",
       "      <td>5.418182</td>\n",
       "      <td>[('i', 'NNS'), ('do', 'VBP'), (\"n't\", 'RB'), (...</td>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>[3.18471347e-04 3.18471350e-04 3.18471338e-04 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you don't want to modify the list in-place ...</td>\n",
       "      <td>1</td>\n",
       "      <td>30549</td>\n",
       "      <td>12</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>102</td>\n",
       "      <td>92</td>\n",
       "      <td>0.162257</td>\n",
       "      <td>567</td>\n",
       "      <td>5.504854</td>\n",
       "      <td>[('if', 'IN'), ('you', 'PRP'), ('do', 'VBP'), ...</td>\n",
       "      <td>54</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[6.57894739e-04 6.57894745e-04 6.57894738e-04 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i assume it helps if the matrix is sparse? yes...</td>\n",
       "      <td>1</td>\n",
       "      <td>8496</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>0.084848</td>\n",
       "      <td>165</td>\n",
       "      <td>6.875000</td>\n",
       "      <td>[('i', 'NN'), ('assume', 'VBP'), ('it', 'PRP')...</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.00172414 0.00172414 0.00172414 0.00172414 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Outcome     Id  \\\n",
       "0  combining lindelof's and gregg lind's ideas: l...        1  15086   \n",
       "1  in most cases r is an interpreted language tha...        1  41061   \n",
       "2  i don't know r at all, but a bit of creative g...        1  34417   \n",
       "3  if you don't want to modify the list in-place ...        1  30549   \n",
       "4  i assume it helps if the matrix is sparse? yes...        1   8496   \n",
       "\n",
       "   num_numbers  prop_numbers  num_words  num_punctuation  prop_punctuation  \\\n",
       "0            3      0.006897         80               51          0.117241   \n",
       "1            0      0.000000         39                4          0.017094   \n",
       "2           12      0.013423        164               49          0.054810   \n",
       "3           12      0.021164        102               92          0.162257   \n",
       "4            0      0.000000         23               14          0.084848   \n",
       "\n",
       "   nchar  word_density                                           pos_tags  \\\n",
       "0    435      5.370370  [('combining', 'VBG'), ('lindelof', 'NN'), (\"'...   \n",
       "1    234      5.850000  [('in', 'IN'), ('most', 'JJS'), ('cases', 'NNS...   \n",
       "2    894      5.418182  [('i', 'NNS'), ('do', 'VBP'), (\"n't\", 'RB'), (...   \n",
       "3    567      5.504854  [('if', 'IN'), ('you', 'PRP'), ('do', 'VBP'), ...   \n",
       "4    165      6.875000  [('i', 'NN'), ('assume', 'VBP'), ('it', 'PRP')...   \n",
       "\n",
       "   noun_count  verb_count  adj_count  adv_count  pron_count  \\\n",
       "0          30           9          9          6           1   \n",
       "1          10           8          7          1           0   \n",
       "2          49          30         14         10           4   \n",
       "3          54          18          8          6           2   \n",
       "4           9           6          0          2           1   \n",
       "\n",
       "                                                 lda  \n",
       "0  [0.00076923 0.00076923 0.00076923 0.00076923 0...  \n",
       "1  [0.00131579 0.00131579 0.00131579 0.00131579 0...  \n",
       "2  [3.18471347e-04 3.18471350e-04 3.18471338e-04 ...  \n",
       "3  [6.57894739e-04 6.57894745e-04 6.57894738e-04 ...  \n",
       "4  [0.00172414 0.00172414 0.00172414 0.00172414 0...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Lowering and replacing only digits was found to be one of the best preprocessing steps for character level model\n",
    "# We replace all digits by 1 to represent all digits, and lowercase the text\n",
    "train[\"Comment\"] = train[\"Comment\"].apply(lambda x: re.sub('\\d', '1', x.lower()))\n",
    "test[\"Comment\"] = test[\"Comment\"].apply(lambda x: re.sub('\\d', '1', x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get train, val and test data\n",
    "def get_train_test(train, test = None, ngram_range = (1,1), max_features=None, random_state=1, test_size=0.2, min_df=1):\n",
    "    \n",
    "    if type(test) != pd.core.frame.DataFrame:\n",
    "        # To check if we want to split into train val, or train test\n",
    "        \n",
    "        # Use only the train data for train val split\n",
    "        X = train.Comment\n",
    "        y = train.Outcome\n",
    "        \n",
    "        # split into train and test set, using random_state so that it is reproducable\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=random_state, test_size=test_size)\n",
    "        \n",
    "        # We use tfidf character level analyser\n",
    "        tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df)\n",
    "\n",
    "        print(\"Fitting...\")\n",
    "        start = time.time()\n",
    "        # Fit transform only on the train set, use it to transform the val set\n",
    "        X_train_dtm =  tfidf_vect_ngram_chars.fit_transform(X_train) \n",
    "        X_val_dtm =  tfidf_vect_ngram_chars.transform(X_val) \n",
    "        print(f\"Operation Took {round(start-time.time(), 2)}s\")\n",
    "        print(X_train_dtm.shape, X_val_dtm.shape)\n",
    "\n",
    "        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n",
    "        add_var_df = train.loc[X_train.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            # Stacks horizontally, effectively increasing columns of features to include our EDA\n",
    "            X_train_dtm = hstack((X_train_dtm, var_sparse))\n",
    "\n",
    "        # Repeat the same for the validation set\n",
    "        add_var_df = train.loc[X_val.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            X_val_dtm = hstack((X_val_dtm, var_sparse))\n",
    "        \n",
    "        print(\"X_train: \", X_train_dtm.shape)\n",
    "        print(\"X_val: \", X_val_dtm.shape)\n",
    "        \n",
    "        return X_train_dtm, X_val_dtm, y_train, y_val\n",
    "    else:\n",
    "        # We use tfidf character level analyser\n",
    "        tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df)\n",
    "\n",
    "        print(\"Fitting...\")\n",
    "        start = time.time()\n",
    "        # Fit on train, transform train and test\n",
    "        X_train_dtm =  tfidf_vect_ngram_chars.fit_transform(train.Comment) \n",
    "        X_test_dtm =  tfidf_vect_ngram_chars.transform(test.Comment) \n",
    "        print(f\"Operation Took {time.time()-start}s\")\n",
    "        print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "\n",
    "        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n",
    "        add_var_df = train[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            # Stacks horizontally, effectively increasing columns of features to include our EDA\n",
    "            X_train_dtm = hstack((X_train_dtm, var_sparse))\n",
    "\n",
    "        # Repeat the same for the test set\n",
    "        add_var_df = test[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            X_test_dtm = hstack((X_test_dtm, var_sparse))\n",
    "        \n",
    "        print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "        \n",
    "        print(\"X_train: \", X_train_dtm.shape)\n",
    "        print(\"X_test: \", X_test_dtm.shape)\n",
    "        \n",
    "        return X_train_dtm, X_test_dtm, train.Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(train = train, test = None, ngram_range = (2,2), min_df = 1):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Vary only the character ngram_range for this notebook\n",
    "    X_train, X_val, y_train, y_val = get_train_test(train = train, test = test, ngram_range = ngram_range, \n",
    "                        max_features=None, random_state=1, test_size=0.1, min_df = min_df)\n",
    "    \n",
    "    LG = LGBMClassifier(class_weight='balanced')\n",
    "    %time LG.fit(X_train, y_train)\n",
    "    \n",
    "    # Metrics for train and validation sets\n",
    "    from sklearn import metrics\n",
    "    print(\"Train\")\n",
    "    y_pred_class = LG.predict(X_train)\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_train, y_pred_class))\n",
    "    # Predict using binary outcomes for additional datapoint\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n",
    "    y_pred_class = LG.predict_proba(X_train)[:, 1]\n",
    "    # Actual AUROC best done using proba\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n",
    "    \n",
    "    y_pred_class = LG.predict(X_val)\n",
    "    print(\"Validation\")\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_val, y_pred_class))\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\n",
    "    y_pred_class = LG.predict_proba(X_val)[:, 1]\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\n",
    "    print(f\"Total Time Elapsed: {round(time.time() - start, 2)}s\")\n",
    "    print(round(metrics.roc_auc_score(y_val, y_pred_class), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore scipy warnings as it was intended to convert to sparse matrix below\n",
    "warnings.filterwarnings(\"ignore\", message=\"Converting data to scipy sparse matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, we optimise for the best ngram range to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -12.4s\n",
      "(40013, 1163) (4446, 1163)\n",
      "X_train:  (40013, 1175)\n",
      "X_val:  (4446, 1175)\n",
      "CPU times: user 7.5 s, sys: 60.4 ms, total: 7.56 s\n",
      "Wall time: 2.13 s\n",
      "Train\n",
      "Accuracy:  0.743983205458226\n",
      "Auroc:  0.7460519603391796\n",
      "Auroc:  0.8274411947132776\n",
      "Validation\n",
      "Accuracy:  0.6675663517768781\n",
      "Auroc:  0.6692684858001128\n",
      "Auroc:  0.7386151208257397\n",
      "Total Time Elapsed: 15.44s\n",
      "0.739\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -25.71s\n",
      "(40013, 7121) (4446, 7121)\n",
      "X_train:  (40013, 7133)\n",
      "X_val:  (4446, 7133)\n",
      "CPU times: user 1min 36s, sys: 653 ms, total: 1min 37s\n",
      "Wall time: 25.7 s\n",
      "Train\n",
      "Accuracy:  0.7912928298303051\n",
      "Auroc:  0.7933687696658086\n",
      "Auroc:  0.8764090736529251\n",
      "Validation\n",
      "Accuracy:  0.7147998200629779\n",
      "Auroc:  0.7158257120005564\n",
      "Auroc:  0.7866412973182964\n",
      "Total Time Elapsed: 54.64s\n",
      "0.787\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -30.85s\n",
      "(40013, 86113) (4446, 86113)\n",
      "X_train:  (40013, 86125)\n",
      "X_val:  (4446, 86125)\n",
      "CPU times: user 5min 47s, sys: 2.16 s, total: 5min 49s\n",
      "Wall time: 1min 32s\n",
      "Train\n",
      "Accuracy:  0.8014395321520505\n",
      "Auroc:  0.8039956399391809\n",
      "Auroc:  0.8888623265432805\n",
      "Validation\n",
      "Accuracy:  0.7206477732793523\n",
      "Auroc:  0.7208305135070793\n",
      "Auroc:  0.7959212606428937\n",
      "Total Time Elapsed: 129.16s\n",
      "0.796\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -38.12s\n",
      "(40013, 475254) (4446, 475254)\n",
      "X_train:  (40013, 475266)\n",
      "X_val:  (4446, 475266)\n",
      "CPU times: user 10min 34s, sys: 3.9 s, total: 10min 38s\n",
      "Wall time: 2min 51s\n",
      "Train\n",
      "Accuracy:  0.7980906205483218\n",
      "Auroc:  0.8012042889208257\n",
      "Auroc:  0.8868079076973918\n",
      "Validation\n",
      "Accuracy:  0.7251461988304093\n",
      "Auroc:  0.7269347128859595\n",
      "Auroc:  0.8080123738331372\n",
      "Total Time Elapsed: 221.02s\n",
      "0.808\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -48.44s\n",
      "(40013, 1359094) (4446, 1359094)\n",
      "X_train:  (40013, 1359106)\n",
      "X_val:  (4446, 1359106)\n",
      "CPU times: user 13min 39s, sys: 4.49 s, total: 13min 43s\n",
      "Wall time: 3min 46s\n",
      "Train\n",
      "Accuracy:  0.7915177567290631\n",
      "Auroc:  0.794765200938423\n",
      "Auroc:  0.8812606472219484\n",
      "Validation\n",
      "Accuracy:  0.7235717498875394\n",
      "Auroc:  0.7257751289986919\n",
      "Auroc:  0.8028956366270893\n",
      "Total Time Elapsed: 288.25s\n",
      "0.803\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -153.94s\n",
      "(40013, 1927582) (4446, 1927582)\n",
      "X_train:  (40013, 1927594)\n",
      "X_val:  (4446, 1927594)\n",
      "CPU times: user 31min 16s, sys: 14.1 s, total: 31min 31s\n",
      "Wall time: 8min 45s\n",
      "Train\n",
      "Accuracy:  0.8095868842626146\n",
      "Auroc:  0.812094808316132\n",
      "Auroc:  0.8986435933498158\n",
      "Validation\n",
      "Accuracy:  0.7393162393162394\n",
      "Auroc:  0.7416080633210376\n",
      "Auroc:  0.8130581224062142\n",
      "Total Time Elapsed: 714.82s\n",
      "0.813\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram boosts performance from (1,1) to (4,4), with slight decrease when using (5,5). (1,1) does poorly so we stick to using (2,5) for range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After finding the optimal ngram range to use, we then optimise for min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -146.44s\n",
      "(40013, 646000) (4446, 646000)\n",
      "X_train:  (40013, 646012)\n",
      "X_val:  (4446, 646012)\n",
      "CPU times: user 31min 20s, sys: 6.53 s, total: 31min 27s\n",
      "Wall time: 8min 39s\n",
      "Train\n",
      "Accuracy:  0.8093119736085772\n",
      "Auroc:  0.811793233956952\n",
      "Auroc:  0.8982669325288872\n",
      "Validation\n",
      "Accuracy:  0.7341430499325237\n",
      "Auroc:  0.7352591074815228\n",
      "Auroc:  0.8137719182548401\n",
      "Total Time Elapsed: 698.73s\n",
      "0.814\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (2,5), min_df = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -146.3s\n",
      "(40013, 441293) (4446, 441293)\n",
      "X_train:  (40013, 441305)\n",
      "X_val:  (4446, 441305)\n",
      "CPU times: user 31min 30s, sys: 6.33 s, total: 31min 37s\n",
      "Wall time: 8min 41s\n",
      "Train\n",
      "Accuracy:  0.8101117136930498\n",
      "Auroc:  0.8125293615047017\n",
      "Auroc:  0.8982312929842518\n",
      "Validation\n",
      "Accuracy:  0.7341430499325237\n",
      "Auroc:  0.7357298958648195\n",
      "Auroc:  0.810908915822502\n",
      "Total Time Elapsed: 699.02s\n",
      "0.811\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (2,5), min_df = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -144.55s\n",
      "(40013, 273048) (4446, 273048)\n",
      "X_train:  (40013, 273060)\n",
      "X_val:  (4446, 273060)\n",
      "CPU times: user 31min 17s, sys: 5.92 s, total: 31min 23s\n",
      "Wall time: 8min 37s\n",
      "Train\n",
      "Accuracy:  0.8124609501911879\n",
      "Auroc:  0.8147251506289197\n",
      "Auroc:  0.8989429086679926\n",
      "Validation\n",
      "Accuracy:  0.7348178137651822\n",
      "Auroc:  0.7366219863524868\n",
      "Auroc:  0.8130891927933986\n",
      "Total Time Elapsed: 691.35s\n",
      "0.813\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (2,5), min_df = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -145.16s\n",
      "(40013, 171098) (4446, 171098)\n",
      "X_train:  (40013, 171110)\n",
      "X_val:  (4446, 171110)\n",
      "CPU times: user 31min 18s, sys: 6.33 s, total: 31min 24s\n",
      "Wall time: 8min 36s\n",
      "Train\n",
      "Accuracy:  0.8110364131657212\n",
      "Auroc:  0.813293790521199\n",
      "Auroc:  0.8981564501943418\n",
      "Validation\n",
      "Accuracy:  0.7368421052631579\n",
      "Auroc:  0.7377681955697742\n",
      "Auroc:  0.8092483991034445\n",
      "Total Time Elapsed: 687.17s\n",
      "0.809\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (2,5), min_df = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -141.61s\n",
      "(40013, 91675) (4446, 91675)\n",
      "X_train:  (40013, 91687)\n",
      "X_val:  (4446, 91687)\n",
      "CPU times: user 31min 7s, sys: 5.59 s, total: 31min 13s\n",
      "Wall time: 8min 32s\n",
      "Train\n",
      "Accuracy:  0.8099617624272112\n",
      "Auroc:  0.8124661223169783\n",
      "Auroc:  0.8985171048213758\n",
      "Validation\n",
      "Accuracy:  0.7388663967611336\n",
      "Auroc:  0.7400325271973915\n",
      "Auroc:  0.8138027828778709\n",
      "Total Time Elapsed: 673.25s\n",
      "0.814\n"
     ]
    }
   ],
   "source": [
    "train_data(ngram_range = (2,5), min_df = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The min df doesn't seem to matter much! min_df of 3 and min_df of 50 does the best with 0.814 AUC. We can use min_df of 50 to reduce the dimensionality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
