{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, our objective is to find the optimal preprocessing steps that would maximize performance on both the train set and validation set (Attempt to minimize both bias and variance). Often times, the best preprocessing might differ from dataset to dataset, and it may even be the case that if we use lesser preprocessing steps, we might obtain a better outcome. We use multiple preprocessing steps to see how well our model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import hyperopt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix, hstack, vstack\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Read in the preprocessed files\n",
    "train = pd.read_csv(\"../datasets/preprocessed_train.csv\")\n",
    "test = pd.read_csv(\"../datasets/preprocessed_test.csv\")\n",
    "train[\"Comment\"] = train[\"Comment\"].apply(lambda x: x.lower())\n",
    "test[\"Comment\"] = test[\"Comment\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get train, val and test data\n",
    "def get_train_test(train, test = None, ngram_range = (1,1), max_features=None, random_state=1, test_size=0.2, min_df=50):\n",
    "    \n",
    "    if type(test) != pd.core.frame.DataFrame:\n",
    "        # To check if we want to split into train val, or train test\n",
    "        \n",
    "        # Use only the train data for train val split\n",
    "        X = train.Processed\n",
    "        y = train.Outcome\n",
    "        \n",
    "        # split into train and test set, using random_state so that it is reproducable\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=random_state, test_size=test_size)\n",
    "        \n",
    "        # We use tfidf character level analyser\n",
    "        tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df)\n",
    "\n",
    "        print(\"Fitting...\")\n",
    "        start = time.time()\n",
    "        # Fit transform only on the train set, use it to transform the val set\n",
    "        X_train_dtm =  tfidf_vect_ngram_chars.fit_transform(X_train) \n",
    "        X_val_dtm =  tfidf_vect_ngram_chars.transform(X_val) \n",
    "        print(f\"Operation Took {round(start-time.time(), 2)}s\")\n",
    "        print(X_train_dtm.shape, X_val_dtm.shape)\n",
    "\n",
    "        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n",
    "        add_var_df = train.loc[X_train.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            # Stacks horizontally, effectively increasing columns of features to include our EDA\n",
    "            X_train_dtm = hstack((X_train_dtm, var_sparse))\n",
    "\n",
    "        # Repeat the same for the validation set\n",
    "        add_var_df = train.loc[X_val.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            X_val_dtm = hstack((X_val_dtm, var_sparse))\n",
    "        \n",
    "        print(\"X_train: \", X_train_dtm.shape)\n",
    "        print(\"X_val: \", X_val_dtm.shape)\n",
    "        \n",
    "        return X_train_dtm, X_val_dtm, y_train, y_val\n",
    "    else:\n",
    "        # We use tfidf character level analyser\n",
    "        tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df)\n",
    "\n",
    "        print(\"Fitting...\")\n",
    "        start = time.time()\n",
    "        # Fit on train, transform train and test\n",
    "        X_train_dtm =  tfidf_vect_ngram_chars.fit_transform(train.Processed) \n",
    "        X_test_dtm =  tfidf_vect_ngram_chars.transform(test.Processed) \n",
    "        print(f\"Operation Took {time.time()-start}s\")\n",
    "        print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "\n",
    "        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n",
    "        add_var_df = train[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            # Stacks horizontally, effectively increasing columns of features to include our EDA\n",
    "            X_train_dtm = hstack((X_train_dtm, var_sparse))\n",
    "\n",
    "        # Repeat the same for the test set\n",
    "        add_var_df = test[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            X_test_dtm = hstack((X_test_dtm, var_sparse))\n",
    "        \n",
    "        print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "        \n",
    "        print(\"X_train: \", X_train_dtm.shape)\n",
    "        print(\"X_test: \", X_test_dtm.shape)\n",
    "        \n",
    "        return X_train_dtm, X_test_dtm, train.Outcome\n",
    "    \n",
    "def train_data(min_df = 50):\n",
    "    start = time.time()\n",
    "    # Using ngram_range of 2,5 because it was found to produce one of the best results for the model\n",
    "    X_train, X_val, y_train, y_val = get_train_test(train, test = None, ngram_range = (2,5), \n",
    "                        max_features=None, random_state=1, test_size=0.1, min_df = min_df)\n",
    "    \n",
    "    LG = LGBMClassifier(class_weight='balanced')\n",
    "    %time LG.fit(X_train, y_train)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    print(\"Train\")\n",
    "    y_pred_class = LG.predict(X_train)\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_train, y_pred_class))\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n",
    "    y_pred_class = LG.predict_proba(X_train)[:, 1]\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n",
    "\n",
    "    y_pred_class = LG.predict(X_val)\n",
    "    print(\"Validation\")\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_val, y_pred_class))\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\n",
    "    y_pred_class = LG.predict_proba(X_val)[:, 1]\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\n",
    "    print(f\"Total Time Elapsed: {round(time.time() - start, 2)}s\")\n",
    "    \n",
    "    return round(metrics.roc_auc_score(y_val, y_pred_class), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore scipy warnings as it was intended to convert to sparse matrix below\n",
    "warnings.filterwarnings(\"ignore\", message=\"Converting data to scipy sparse matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -155.85s\n",
      "(40013, 96827) (4446, 96827)\n",
      "X_train:  (40013, 96839)\n",
      "X_val:  (4446, 96839)\n",
      "CPU times: user 32min 18s, sys: 13.9 s, total: 32min 32s\n",
      "Wall time: 8min 56s\n",
      "Train\n",
      "Accuracy:  0.8104615999800066\n",
      "Auroc:  0.8129102903901771\n",
      "Auroc:  0.8986492561818136\n",
      "Validation\n",
      "Accuracy:  0.7361673414304993\n",
      "Auroc:  0.7384650158757331\n",
      "Auroc:  0.813580146063742\n",
      "Total Time Elapsed: 715.02s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.814"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"]\n",
    "test[\"Processed\"] = test[\"Comment\"]\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -156.31s\n",
      "(40013, 108120) (4446, 108120)\n",
      "X_train:  (40013, 108132)\n",
      "X_val:  (4446, 108132)\n",
      "CPU times: user 31min 51s, sys: 6.34 s, total: 31min 57s\n",
      "Wall time: 8min 42s\n",
      "Train\n",
      "Accuracy:  0.8065878589458426\n",
      "Auroc:  0.809124296309362\n",
      "Auroc:  0.8944642469271319\n",
      "Validation\n",
      "Accuracy:  0.728969860548808\n",
      "Auroc:  0.7311463964626673\n",
      "Auroc:  0.8053403205352831\n",
      "Total Time Elapsed: 700.74s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.805"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: \"\".join(x.split()))\n",
    "test[\"Processed\"] = test[\"Comment\"].apply(lambda x: \"\".join(x.split()))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -145.74s\n",
      "(40013, 91675) (4446, 91675)\n",
      "X_train:  (40013, 91687)\n",
      "X_val:  (4446, 91687)\n",
      "CPU times: user 31min 36s, sys: 6.31 s, total: 31min 43s\n",
      "Wall time: 8min 42s\n",
      "Train\n",
      "Accuracy:  0.8099617624272112\n",
      "Auroc:  0.8124661223169783\n",
      "Auroc:  0.8985171048213758\n",
      "Validation\n",
      "Accuracy:  0.7388663967611336\n",
      "Auroc:  0.7400325271973915\n",
      "Auroc:  0.8138027828778709\n",
      "Total Time Elapsed: 689.41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.814"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: re.sub('\\d', '1', x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -121.96s\n",
      "(40013, 90352) (4446, 90352)\n",
      "X_train:  (40013, 90364)\n",
      "X_val:  (4446, 90364)\n",
      "CPU times: user 26min 31s, sys: 5.9 s, total: 26min 37s\n",
      "Wall time: 7min 8s\n",
      "Train\n",
      "Accuracy:  0.8069877289880789\n",
      "Auroc:  0.8093777962453956\n",
      "Auroc:  0.8923867985542524\n",
      "Validation\n",
      "Accuracy:  0.7307692307692307\n",
      "Auroc:  0.7332114455075822\n",
      "Auroc:  0.8113714736396571\n",
      "Total Time Elapsed: 566.39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.811"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: remove_stopwords(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "# Helper function to lematize \n",
    "def lemm_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemma_word1 = [] \n",
    "    for token in doc:\n",
    "        lemma_word1.append(token.lemma_)\n",
    "    return \"\".join(lemma_word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -146.53s\n",
      "(40013, 104201) (4446, 104201)\n",
      "X_train:  (40013, 104213)\n",
      "X_val:  (4446, 104213)\n",
      "CPU times: user 30min 48s, sys: 12.7 s, total: 31min 1s\n",
      "Wall time: 8min 22s\n",
      "Train\n",
      "Accuracy:  0.8037137930172694\n",
      "Auroc:  0.8064239427623381\n",
      "Auroc:  0.8917881501501606\n",
      "Validation\n",
      "Accuracy:  0.7190733243364822\n",
      "Auroc:  0.7205536578384928\n",
      "Auroc:  0.8016059069127084\n",
      "Total Time Elapsed: 671.41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits, Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -114.96s\n",
      "(40013, 85433) (4446, 85433)\n",
      "X_train:  (40013, 85445)\n",
      "X_val:  (4446, 85445)\n",
      "CPU times: user 25min 48s, sys: 5.27 s, total: 25min 54s\n",
      "Wall time: 6min 54s\n",
      "Train\n",
      "Accuracy:  0.8057131432284508\n",
      "Auroc:  0.8081051451902659\n",
      "Auroc:  0.8923377545202484\n",
      "Validation\n",
      "Accuracy:  0.7350427350427351\n",
      "Auroc:  0.7369978145789253\n",
      "Auroc:  0.8108541825576607\n",
      "Total Time Elapsed: 545.39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.811"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: remove_stopwords(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -125.08s\n",
      "(40013, 90770) (4446, 90770)\n",
      "X_train:  (40013, 90782)\n",
      "X_val:  (4446, 90782)\n",
      "CPU times: user 22min 53s, sys: 10.2 s, total: 23min 3s\n",
      "Wall time: 6min 9s\n",
      "Train\n",
      "Accuracy:  0.7985904581011172\n",
      "Auroc:  0.8011010964354499\n",
      "Auroc:  0.8857950971225945\n",
      "Validation\n",
      "Accuracy:  0.7188484030589294\n",
      "Auroc:  0.7208840121869993\n",
      "Auroc:  0.8003377824344492\n",
      "Total Time Elapsed: 508.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(remove_stopwords(x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -139.31s\n",
      "(40013, 101656) (4446, 101656)\n",
      "X_train:  (40013, 101668)\n",
      "X_val:  (4446, 101668)\n",
      "CPU times: user 30min 13s, sys: 13.1 s, total: 30min 26s\n",
      "Wall time: 8min 12s\n",
      "Train\n",
      "Accuracy:  0.8043385899582636\n",
      "Auroc:  0.8067754838087855\n",
      "Auroc:  0.8920877979788671\n",
      "Validation\n",
      "Accuracy:  0.725820962663068\n",
      "Auroc:  0.7276502577298906\n",
      "Auroc:  0.797522517285732\n",
      "Total Time Elapsed: 653.76s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.798"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Digits, Stopwords, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -116.54s\n",
      "(40013, 88374) (4446, 88374)\n",
      "X_train:  (40013, 88386)\n",
      "X_val:  (4446, 88386)\n",
      "CPU times: user 22min 29s, sys: 9.79 s, total: 22min 39s\n",
      "Wall time: 6min 3s\n",
      "Train\n",
      "Accuracy:  0.7986654337340364\n",
      "Auroc:  0.8014286726104015\n",
      "Auroc:  0.886496283143996\n",
      "Validation\n",
      "Accuracy:  0.7235717498875394\n",
      "Auroc:  0.7253043406153953\n",
      "Auroc:  0.7984040109194921\n",
      "Total Time Elapsed: 493.33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.798"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(remove_stopwords(re.sub('\\d', '1', x))))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stem_text(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return \"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -142.89s\n",
      "(40013, 107611) (4446, 107611)\n",
      "X_train:  (40013, 107623)\n",
      "X_val:  (4446, 107623)\n",
      "CPU times: user 31min 47s, sys: 9.35 s, total: 31min 56s\n",
      "Wall time: 8min 36s\n",
      "Train\n",
      "Accuracy:  0.8063129482918051\n",
      "Auroc:  0.8087654400312615\n",
      "Auroc:  0.8937823376325827\n",
      "Validation\n",
      "Accuracy:  0.7217723796671165\n",
      "Auroc:  0.7241808683370736\n",
      "Auroc:  0.8022929534213743\n",
      "Total Time Elapsed: 681.69s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Remove Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -131.78s\n",
      "(40013, 105020) (4446, 105020)\n",
      "X_train:  (40013, 105032)\n",
      "X_val:  (4446, 105032)\n",
      "CPU times: user 31min 4s, sys: 6.69 s, total: 31min 11s\n",
      "Wall time: 8min 23s\n",
      "Train\n",
      "Accuracy:  0.8081123634818684\n",
      "Auroc:  0.8104280916713559\n",
      "Auroc:  0.8939054299847731\n",
      "Validation\n",
      "Accuracy:  0.7235717498875394\n",
      "Auroc:  0.7248335522320986\n",
      "Auroc:  0.8023050935064331\n",
      "Total Time Elapsed: 657.14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -123.22s\n",
      "(40013, 91252) (4446, 91252)\n",
      "X_train:  (40013, 91264)\n",
      "X_val:  (4446, 91264)\n",
      "CPU times: user 21min 57s, sys: 4.83 s, total: 22min 2s\n",
      "Wall time: 5min 56s\n",
      "Train\n",
      "Accuracy:  0.798065628670682\n",
      "Auroc:  0.80093385886851\n",
      "Auroc:  0.8867457828006802\n",
      "Validation\n",
      "Accuracy:  0.7147998200629779\n",
      "Auroc:  0.7173557742462705\n",
      "Auroc:  0.7939500400519925\n",
      "Total Time Elapsed: 493.39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.794"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(remove_stopwords(x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Stopwords Removal, Digits Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -112.2s\n",
      "(40013, 88806) (4446, 88806)\n",
      "X_train:  (40013, 88818)\n",
      "X_val:  (4446, 88818)\n",
      "CPU times: user 21min 49s, sys: 5.08 s, total: 21min 54s\n",
      "Wall time: 5min 54s\n",
      "Train\n",
      "Accuracy:  0.7980906205483218\n",
      "Auroc:  0.8008415034343286\n",
      "Auroc:  0.886016343537249\n",
      "Validation\n",
      "Accuracy:  0.717948717948718\n",
      "Auroc:  0.7204399731436627\n",
      "Auroc:  0.7968336188996844\n",
      "Total Time Elapsed: 479.95s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.797"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(remove_stopwords(re.sub('\\d', '1', x))))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
