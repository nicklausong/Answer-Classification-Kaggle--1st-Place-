{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer, Preprocessing Variations, Binary Count\n",
    "## In this notebook, our objective is to find the optimal preprocessing steps that would maximize performance on both the train set and validation set (Attempt to minimize both bias and variance). Often times, the best preprocessing might differ from dataset to dataset, and it may even be the case that if we use lesser preprocessing steps, we might obtain a better outcome. We use multiple preprocessing steps to see how well our model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import hyperopt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix, hstack, vstack\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Read in the preprocessed files\n",
    "train = pd.read_csv(\"../datasets/preprocessed_train.csv\")\n",
    "test = pd.read_csv(\"../datasets/preprocessed_test.csv\")\n",
    "train[\"Comment\"] = train[\"Comment\"].apply(lambda x: x.lower())\n",
    "test[\"Comment\"] = test[\"Comment\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get train, val and test data\n",
    "def get_train_test(train, test = None, ngram_range = (1,1), max_features=None, random_state=1, test_size=0.2, min_df=50):\n",
    "    \n",
    "    if type(test) != pd.core.frame.DataFrame:\n",
    "        # To check if we want to split into train val, or train test\n",
    "        \n",
    "        # Use only the train data for train val split\n",
    "        X = train.Processed\n",
    "        y = train.Outcome\n",
    "        \n",
    "        # split into train and test set, using random_state so that it is reproducable\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=random_state, test_size=test_size)\n",
    "        \n",
    "        # We use count vect character level analyser\n",
    "        # Binary set to true\n",
    "        count_vect_ngram_chars = CountVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df, binary = True)\n",
    "\n",
    "        print(\"Fitting...\")\n",
    "        start = time.time()\n",
    "        # Fit transform only on the train set, use it to transform the val set\n",
    "        X_train_dtm =  count_vect_ngram_chars.fit_transform(X_train) \n",
    "        X_val_dtm =  count_vect_ngram_chars.transform(X_val) \n",
    "        print(f\"Operation Took {round(start-time.time(), 2)}s\")\n",
    "        print(X_train_dtm.shape, X_val_dtm.shape)\n",
    "\n",
    "        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n",
    "        add_var_df = train.loc[X_train.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            # Stacks horizontally, effectively increasing columns of features to include our EDA\n",
    "            X_train_dtm = hstack((X_train_dtm, var_sparse))\n",
    "\n",
    "        # Repeat the same for the validation set\n",
    "        add_var_df = train.loc[X_val.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            X_val_dtm = hstack((X_val_dtm, var_sparse))\n",
    "        \n",
    "        print(\"X_train: \", X_train_dtm.shape)\n",
    "        print(\"X_val: \", X_val_dtm.shape)\n",
    "        \n",
    "        return X_train_dtm, X_val_dtm, y_train, y_val\n",
    "    else:\n",
    "        # We use ccount vect character level analyser\n",
    "        # Binary set to true\n",
    "        count_vect_ngram_chars = CountVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df, binary = True)\n",
    "\n",
    "        print(\"Fitting...\")\n",
    "        start = time.time()\n",
    "        # Fit on train, transform train and test\n",
    "        X_train_dtm =  count_vect_ngram_chars.fit_transform(train.Processed) \n",
    "        X_test_dtm =  count_vect_ngram_chars.transform(test.Processed) \n",
    "        print(f\"Operation Took {time.time()-start}s\")\n",
    "        print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "\n",
    "        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n",
    "        add_var_df = train[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            # Stacks horizontally, effectively increasing columns of features to include our EDA\n",
    "            X_train_dtm = hstack((X_train_dtm, var_sparse))\n",
    "\n",
    "        # Repeat the same for the test set\n",
    "        add_var_df = test[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            X_test_dtm = hstack((X_test_dtm, var_sparse))\n",
    "        \n",
    "        print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "        \n",
    "        print(\"X_train: \", X_train_dtm.shape)\n",
    "        print(\"X_test: \", X_test_dtm.shape)\n",
    "        \n",
    "        return X_train_dtm, X_test_dtm, train.Outcome\n",
    "    \n",
    "def train_data(min_df = 50):\n",
    "    start = time.time()\n",
    "    # Using ngram_range of 2,5 because it was found to produce one of the best results for the model\n",
    "    X_train, X_val, y_train, y_val = get_train_test(train, test = None, ngram_range = (2,5), \n",
    "                        max_features=None, random_state=1, test_size=0.1, min_df = min_df)\n",
    "    \n",
    "    LG = LGBMClassifier(class_weight='balanced')\n",
    "    %time LG.fit(X_train, y_train)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    print(\"Train\")\n",
    "    y_pred_class = LG.predict(X_train)\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_train, y_pred_class))\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n",
    "    y_pred_class = LG.predict_proba(X_train)[:, 1]\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n",
    "\n",
    "    y_pred_class = LG.predict(X_val)\n",
    "    print(\"Validation\")\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_val, y_pred_class))\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\n",
    "    y_pred_class = LG.predict_proba(X_val)[:, 1]\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\n",
    "    print(f\"Total Time Elapsed: {round(time.time() - start, 2)}s\")\n",
    "    \n",
    "    return round(metrics.roc_auc_score(y_val, y_pred_class), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore scipy warnings as it was intended to convert to sparse matrix below\n",
    "warnings.filterwarnings(\"ignore\", message=\"Converting data to scipy sparse matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -134.14s\n",
      "(40013, 96827) (4446, 96827)\n",
      "X_train:  (40013, 96839)\n",
      "X_val:  (4446, 96839)\n",
      "CPU times: user 13min 44s, sys: 2.12 s, total: 13min 46s\n",
      "Wall time: 3min 56s\n",
      "Train\n",
      "Accuracy:  0.7929422937545297\n",
      "Auroc:  0.7952545917127827\n",
      "Auroc:  0.8816791574120229\n",
      "Validation\n",
      "Accuracy:  0.7343679712100765\n",
      "Auroc:  0.7361645726391701\n",
      "Auroc:  0.8135503102614787\n",
      "Total Time Elapsed: 391.76s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.814"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"]\n",
    "test[\"Processed\"] = test[\"Comment\"]\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -137.05s\n",
      "(40013, 108120) (4446, 108120)\n",
      "X_train:  (40013, 108132)\n",
      "X_val:  (4446, 108132)\n",
      "CPU times: user 13min 9s, sys: 1.65 s, total: 13min 11s\n",
      "Wall time: 3min 47s\n",
      "Train\n",
      "Accuracy:  0.7890935445980056\n",
      "Auroc:  0.7920127079635708\n",
      "Auroc:  0.8777334719765177\n",
      "Validation\n",
      "Accuracy:  0.7285200179937023\n",
      "Auroc:  0.7322778935429769\n",
      "Auroc:  0.806675524127596\n",
      "Total Time Elapsed: 386.28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.807"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: \"\".join(x.split()))\n",
    "test[\"Processed\"] = test[\"Comment\"].apply(lambda x: \"\".join(x.split()))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -126.89s\n",
      "(40013, 91675) (4446, 91675)\n",
      "X_train:  (40013, 91687)\n",
      "X_val:  (4446, 91687)\n",
      "CPU times: user 12min 43s, sys: 1.65 s, total: 12min 45s\n",
      "Wall time: 3min 38s\n",
      "Train\n",
      "Accuracy:  0.7945167820458351\n",
      "Auroc:  0.7968764841613839\n",
      "Auroc:  0.882032042460955\n",
      "Validation\n",
      "Accuracy:  0.7379667116509222\n",
      "Auroc:  0.7392942454144945\n",
      "Auroc:  0.814373366875634\n",
      "Total Time Elapsed: 365.53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.814"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: re.sub('\\d', '1', x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -107.27s\n",
      "(40013, 90352) (4446, 90352)\n",
      "X_train:  (40013, 90364)\n",
      "X_val:  (4446, 90364)\n",
      "CPU times: user 10min 39s, sys: 1.42 s, total: 10min 41s\n",
      "Wall time: 2min 58s\n",
      "Train\n",
      "Accuracy:  0.7887936420663284\n",
      "Auroc:  0.7914470682097328\n",
      "Auroc:  0.8768147136549023\n",
      "Validation\n",
      "Accuracy:  0.7285200179937023\n",
      "Auroc:  0.7318071051596803\n",
      "Auroc:  0.8109424553795289\n",
      "Total Time Elapsed: 300.52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.811"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: remove_stopwords(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "# Helper function to lematize \n",
    "def lemm_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemma_word1 = [] \n",
    "    for token in doc:\n",
    "        lemma_word1.append(token.lemma_)\n",
    "    return \"\".join(lemma_word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -126.27s\n",
      "(40013, 104201) (4446, 104201)\n",
      "X_train:  (40013, 104213)\n",
      "X_val:  (4446, 104213)\n",
      "CPU times: user 12min 42s, sys: 1.74 s, total: 12min 44s\n",
      "Wall time: 3min 36s\n",
      "Train\n",
      "Accuracy:  0.7866693324669483\n",
      "Auroc:  0.7896420944481894\n",
      "Auroc:  0.8747179555579423\n",
      "Validation\n",
      "Accuracy:  0.7240215924426451\n",
      "Auroc:  0.726585633999481\n",
      "Auroc:  0.8023022128082836\n",
      "Total Time Elapsed: 362.95s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits, Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -103.55s\n",
      "(40013, 85433) (4446, 85433)\n",
      "X_train:  (40013, 85445)\n",
      "X_val:  (4446, 85445)\n",
      "CPU times: user 10min 30s, sys: 1.42 s, total: 10min 31s\n",
      "Wall time: 2min 55s\n",
      "Train\n",
      "Accuracy:  0.7877189913278184\n",
      "Auroc:  0.790294802464962\n",
      "Auroc:  0.8767804549173518\n",
      "Validation\n",
      "Accuracy:  0.728969860548808\n",
      "Auroc:  0.7310875479147553\n",
      "Auroc:  0.8087432481065069\n",
      "Total Time Elapsed: 294.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.809"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: remove_stopwords(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -108.58s\n",
      "(40013, 90770) (4446, 90770)\n",
      "X_train:  (40013, 90782)\n",
      "X_val:  (4446, 90782)\n",
      "CPU times: user 9min 1s, sys: 1.17 s, total: 9min 2s\n",
      "Wall time: 2min 31s\n",
      "Train\n",
      "Accuracy:  0.7820458351035914\n",
      "Auroc:  0.7847888748251318\n",
      "Auroc:  0.869847581183626\n",
      "Validation\n",
      "Accuracy:  0.7161493477282951\n",
      "Auroc:  0.719198803769517\n",
      "Auroc:  0.7987066899893476\n",
      "Total Time Elapsed: 272.61s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.799"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(remove_stopwords(x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -118.01s\n",
      "(40013, 101656) (4446, 101656)\n",
      "X_train:  (40013, 101668)\n",
      "X_val:  (4446, 101668)\n",
      "CPU times: user 12min 21s, sys: 1.68 s, total: 12min 23s\n",
      "Wall time: 3min 30s\n",
      "Train\n",
      "Accuracy:  0.7861944867917927\n",
      "Auroc:  0.7891246649137829\n",
      "Auroc:  0.8746406303611769\n",
      "Validation\n",
      "Accuracy:  0.725820962663068\n",
      "Auroc:  0.7288860772360443\n",
      "Auroc:  0.800884292026248\n",
      "Total Time Elapsed: 348.07s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.801"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Digits, Stopwords, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -97.08s\n",
      "(40013, 88374) (4446, 88374)\n",
      "X_train:  (40013, 88386)\n",
      "X_val:  (4446, 88386)\n",
      "CPU times: user 8min 45s, sys: 1.4 s, total: 8min 46s\n",
      "Wall time: 2min 27s\n",
      "Train\n",
      "Accuracy:  0.781870891960113\n",
      "Auroc:  0.7847798031256425\n",
      "Auroc:  0.8702119911490469\n",
      "Validation\n",
      "Accuracy:  0.7123256860098965\n",
      "Auroc:  0.7152225143844576\n",
      "Auroc:  0.796711189228329\n",
      "Total Time Elapsed: 256.48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.797"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(remove_stopwords(re.sub('\\d', '1', x))))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stem_text(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return \"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -121.85s\n",
      "(40013, 107611) (4446, 107611)\n",
      "X_train:  (40013, 107623)\n",
      "X_val:  (4446, 107623)\n",
      "CPU times: user 12min 3s, sys: 1.64 s, total: 12min 5s\n",
      "Wall time: 3min 25s\n",
      "Train\n",
      "Accuracy:  0.7877189913278184\n",
      "Auroc:  0.7909758208343514\n",
      "Auroc:  0.8768177290939043\n",
      "Validation\n",
      "Accuracy:  0.7228969860548808\n",
      "Auroc:  0.7253538268943213\n",
      "Auroc:  0.8019341007376026\n",
      "Total Time Elapsed: 346.66s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Remove Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -117.24s\n",
      "(40013, 105020) (4446, 105020)\n",
      "X_train:  (40013, 105032)\n",
      "X_val:  (4446, 105032)\n",
      "CPU times: user 12min 18s, sys: 1.82 s, total: 12min 20s\n",
      "Wall time: 3min 29s\n",
      "Train\n",
      "Accuracy:  0.7891185364756454\n",
      "Auroc:  0.7919712697906522\n",
      "Auroc:  0.8772379716384561\n",
      "Validation\n",
      "Accuracy:  0.7192982456140351\n",
      "Auroc:  0.7221653055710849\n",
      "Auroc:  0.8001145283278596\n",
      "Total Time Elapsed: 346.33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -102.74s\n",
      "(40013, 91252) (4446, 91252)\n",
      "X_train:  (40013, 91264)\n",
      "X_val:  (4446, 91264)\n",
      "CPU times: user 8min 21s, sys: 1.23 s, total: 8min 22s\n",
      "Wall time: 2min 21s\n",
      "Train\n",
      "Accuracy:  0.7825456726563866\n",
      "Auroc:  0.7858949672947465\n",
      "Auroc:  0.8707028147645824\n",
      "Validation\n",
      "Accuracy:  0.7147998200629779\n",
      "Auroc:  0.7175323198900067\n",
      "Auroc:  0.7949173373377781\n",
      "Total Time Elapsed: 255.45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.795"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(remove_stopwords(x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Stopwords Removal, Digits Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -93.4s\n",
      "(40013, 88806) (4446, 88806)\n",
      "X_train:  (40013, 88818)\n",
      "X_val:  (4446, 88818)\n",
      "CPU times: user 8min 17s, sys: 1.02 s, total: 8min 18s\n",
      "Wall time: 2min 19s\n",
      "Train\n",
      "Accuracy:  0.7803963711793667\n",
      "Auroc:  0.7837049996430462\n",
      "Auroc:  0.8703654536423642\n",
      "Validation\n",
      "Accuracy:  0.711650922177238\n",
      "Auroc:  0.7143892724447023\n",
      "Auroc:  0.7930860363712833\n",
      "Total Time Elapsed: 244.77s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.793"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(remove_stopwords(re.sub('\\d', '1', x))))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
