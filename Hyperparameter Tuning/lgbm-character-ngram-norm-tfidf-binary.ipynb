{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, our objective is to find the optimal preprocessing steps that would maximize performance on both the train set and validation set (Attempt to minimize both bias and variance). Often times, the best preprocessing might differ from dataset to dataset, and it may even be the case that if we use lesser preprocessing steps, we might obtain a better outcome. We use multiple preprocessing steps to see how well our model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import hyperopt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix, hstack, vstack\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Read in the preprocessed files\n",
    "train = pd.read_csv(\"../datasets/preprocessed_train.csv\")\n",
    "test = pd.read_csv(\"../datasets/preprocessed_test.csv\")\n",
    "train[\"Comment\"] = train[\"Comment\"].apply(lambda x: x.lower())\n",
    "test[\"Comment\"] = test[\"Comment\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get train, val and test data\n",
    "def get_train_test(train, test = None, ngram_range = (1,1), max_features=None, random_state=1, test_size=0.2, min_df=50):\n",
    "    \n",
    "    if type(test) != pd.core.frame.DataFrame:\n",
    "        # To check if we want to split into train val, or train test\n",
    "        \n",
    "        # Use only the train data for train val split\n",
    "        X = train.Processed\n",
    "        y = train.Outcome\n",
    "        \n",
    "        # split into train and test set, using random_state so that it is reproducable\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=random_state, test_size=test_size)\n",
    "        \n",
    "        # We use tfidf character level analyser\n",
    "        tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df, binary = True)\n",
    "\n",
    "        print(\"Fitting...\")\n",
    "        start = time.time()\n",
    "        # Fit transform only on the train set, use it to transform the val set\n",
    "        X_train_dtm =  tfidf_vect_ngram_chars.fit_transform(X_train) \n",
    "        X_val_dtm =  tfidf_vect_ngram_chars.transform(X_val) \n",
    "        print(f\"Operation Took {round(start-time.time(), 2)}s\")\n",
    "        print(X_train_dtm.shape, X_val_dtm.shape)\n",
    "\n",
    "        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n",
    "        add_var_df = train.loc[X_train.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            # Stacks horizontally, effectively increasing columns of features to include our EDA\n",
    "            X_train_dtm = hstack((X_train_dtm, var_sparse))\n",
    "\n",
    "        # Repeat the same for the validation set\n",
    "        add_var_df = train.loc[X_val.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            X_val_dtm = hstack((X_val_dtm, var_sparse))\n",
    "        \n",
    "        print(\"X_train: \", X_train_dtm.shape)\n",
    "        print(\"X_val: \", X_val_dtm.shape)\n",
    "        \n",
    "        return X_train_dtm, X_val_dtm, y_train, y_val\n",
    "    else:\n",
    "        # We use tfidf character level analyser\n",
    "        tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df, binary = True)\n",
    "\n",
    "        print(\"Fitting...\")\n",
    "        start = time.time()\n",
    "        # Fit on train, transform train and test\n",
    "        X_train_dtm =  tfidf_vect_ngram_chars.fit_transform(train.Processed) \n",
    "        X_test_dtm =  tfidf_vect_ngram_chars.transform(test.Processed) \n",
    "        print(f\"Operation Took {time.time()-start}s\")\n",
    "        print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "\n",
    "        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n",
    "        add_var_df = train[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            # Stacks horizontally, effectively increasing columns of features to include our EDA\n",
    "            X_train_dtm = hstack((X_train_dtm, var_sparse))\n",
    "\n",
    "        # Repeat the same for the test set\n",
    "        add_var_df = test[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            X_test_dtm = hstack((X_test_dtm, var_sparse))\n",
    "        \n",
    "        print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "        \n",
    "        print(\"X_train: \", X_train_dtm.shape)\n",
    "        print(\"X_test: \", X_test_dtm.shape)\n",
    "        \n",
    "        return X_train_dtm, X_test_dtm, train.Outcome\n",
    "    \n",
    "def train_data(min_df = 50):\n",
    "    start = time.time()\n",
    "    # Using ngram_range of 2,5 because it was found to produce one of the best results for the model\n",
    "    X_train, X_val, y_train, y_val = get_train_test(train, test = None, ngram_range = (2,5), \n",
    "                        max_features=None, random_state=1, test_size=0.1, min_df = min_df)\n",
    "    \n",
    "    LG = LGBMClassifier(class_weight='balanced')\n",
    "    %time LG.fit(X_train, y_train)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    print(\"Train\")\n",
    "    y_pred_class = LG.predict(X_train)\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_train, y_pred_class))\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n",
    "    y_pred_class = LG.predict_proba(X_train)[:, 1]\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n",
    "\n",
    "    y_pred_class = LG.predict(X_val)\n",
    "    print(\"Validation\")\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_val, y_pred_class))\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\n",
    "    y_pred_class = LG.predict_proba(X_val)[:, 1]\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\n",
    "    print(f\"Total Time Elapsed: {round(time.time() - start, 2)}s\")\n",
    "    \n",
    "    return round(metrics.roc_auc_score(y_val, y_pred_class), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore scipy warnings as it was intended to convert to sparse matrix below\n",
    "warnings.filterwarnings(\"ignore\", message=\"Converting data to scipy sparse matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -150.42s\n",
      "(40013, 96827) (4446, 96827)\n",
      "X_train:  (40013, 96839)\n",
      "X_val:  (4446, 96839)\n",
      "CPU times: user 32min 28s, sys: 14.4 s, total: 32min 42s\n",
      "Wall time: 8min 55s\n",
      "Train\n",
      "Accuracy:  0.8099117786719316\n",
      "Auroc:  0.8118934389240573\n",
      "Auroc:  0.8967581748843243\n",
      "Validation\n",
      "Accuracy:  0.7327935222672065\n",
      "Auroc:  0.7349461402039905\n",
      "Auroc:  0.8127328092794694\n",
      "Total Time Elapsed: 708.35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.813"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"]\n",
    "test[\"Processed\"] = test[\"Comment\"]\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -144.01s\n",
      "(40013, 108120) (4446, 108120)\n",
      "X_train:  (40013, 108132)\n",
      "X_val:  (4446, 108132)\n",
      "CPU times: user 32min 6s, sys: 6.47 s, total: 32min 12s\n",
      "Wall time: 8min 45s\n",
      "Train\n",
      "Accuracy:  0.8050633544098168\n",
      "Auroc:  0.8072222231275308\n",
      "Auroc:  0.8929065480476218\n",
      "Validation\n",
      "Accuracy:  0.7215474583895637\n",
      "Auroc:  0.7239815857543714\n",
      "Auroc:  0.802298920581827\n",
      "Total Time Elapsed: 691.37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: \"\".join(x.split()))\n",
    "test[\"Processed\"] = test[\"Comment\"].apply(lambda x: \"\".join(x.split()))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -139.35s\n",
      "(40013, 91675) (4446, 91675)\n",
      "X_train:  (40013, 91687)\n",
      "X_val:  (4446, 91687)\n",
      "CPU times: user 31min 39s, sys: 6.48 s, total: 31min 46s\n",
      "Wall time: 8min 38s\n",
      "Train\n",
      "Accuracy:  0.8099867543048509\n",
      "Auroc:  0.8119091468737744\n",
      "Auroc:  0.8968964484997057\n",
      "Validation\n",
      "Accuracy:  0.7305443094916779\n",
      "Auroc:  0.7325413745415831\n",
      "Auroc:  0.8095990212210743\n",
      "Total Time Elapsed: 679.66s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: re.sub('\\d', '1', x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -119.86s\n",
      "(40013, 90352) (4446, 90352)\n",
      "X_train:  (40013, 90364)\n",
      "X_val:  (4446, 90364)\n",
      "CPU times: user 26min 42s, sys: 5.53 s, total: 26min 47s\n",
      "Wall time: 7min 8s\n",
      "Train\n",
      "Accuracy:  0.8042136305700647\n",
      "Auroc:  0.8062698330156995\n",
      "Auroc:  0.8916762514730533\n",
      "Validation\n",
      "Accuracy:  0.7402159244264508\n",
      "Auroc:  0.7417578596248138\n",
      "Auroc:  0.8102819524466696\n",
      "Total Time Elapsed: 564.46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: remove_stopwords(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "# Helper function to lematize \n",
    "def lemm_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemma_word1 = [] \n",
    "    for token in doc:\n",
    "        lemma_word1.append(token.lemma_)\n",
    "    return \"\".join(lemma_word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -143.38s\n",
      "(40013, 104201) (4446, 104201)\n",
      "X_train:  (40013, 104213)\n",
      "X_val:  (4446, 104213)\n",
      "CPU times: user 31min 12s, sys: 13.6 s, total: 31min 25s\n",
      "Wall time: 8min 29s\n",
      "Train\n",
      "Accuracy:  0.8025641666458401\n",
      "Auroc:  0.8045303980948557\n",
      "Auroc:  0.8905412077431216\n",
      "Validation\n",
      "Accuracy:  0.7154745838956366\n",
      "Auroc:  0.7170120452277841\n",
      "Auroc:  0.798011001386233\n",
      "Total Time Elapsed: 675.67s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.798"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits, Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -118.4s\n",
      "(40013, 85433) (4446, 85433)\n",
      "X_train:  (40013, 85445)\n",
      "X_val:  (4446, 85445)\n",
      "CPU times: user 26min 2s, sys: 5.81 s, total: 26min 8s\n",
      "Wall time: 6min 57s\n",
      "Train\n",
      "Accuracy:  0.8050883462874566\n",
      "Auroc:  0.807193514269928\n",
      "Auroc:  0.8918406766608569\n",
      "Validation\n",
      "Accuracy:  0.7343679712100765\n",
      "Auroc:  0.7354583900642252\n",
      "Auroc:  0.807373681900553\n",
      "Total Time Elapsed: 552.39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.807"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: remove_stopwords(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -123.67s\n",
      "(40013, 90770) (4446, 90770)\n",
      "X_train:  (40013, 90782)\n",
      "X_val:  (4446, 90782)\n",
      "CPU times: user 23min 12s, sys: 10.9 s, total: 23min 23s\n",
      "Wall time: 6min 14s\n",
      "Train\n",
      "Accuracy:  0.7982155799365206\n",
      "Auroc:  0.8001569632453978\n",
      "Auroc:  0.8854646400358018\n",
      "Validation\n",
      "Accuracy:  0.7215474583895637\n",
      "Auroc:  0.722981160439866\n",
      "Auroc:  0.7992211003731944\n",
      "Total Time Elapsed: 512.76s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.799"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(remove_stopwords(x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -139.2s\n",
      "(40013, 101656) (4446, 101656)\n",
      "X_train:  (40013, 101668)\n",
      "X_val:  (4446, 101668)\n",
      "CPU times: user 30min 55s, sys: 13.1 s, total: 31min 8s\n",
      "Wall time: 8min 24s\n",
      "Train\n",
      "Accuracy:  0.8025641666458401\n",
      "Auroc:  0.8046895145363019\n",
      "Auroc:  0.8899540677569042\n",
      "Validation\n",
      "Accuracy:  0.7244714349977508\n",
      "Auroc:  0.725042197083787\n",
      "Auroc:  0.7993285092613417\n",
      "Total Time Elapsed: 666.37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.799"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Digits, Stopwords, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -116.68s\n",
      "(40013, 88374) (4446, 88374)\n",
      "X_train:  (40013, 88386)\n",
      "X_val:  (4446, 88386)\n",
      "CPU times: user 22min 45s, sys: 10.6 s, total: 22min 56s\n",
      "Wall time: 6min 7s\n",
      "Train\n",
      "Accuracy:  0.7975907829955264\n",
      "Auroc:  0.799652670415162\n",
      "Auroc:  0.8845944701326174\n",
      "Validation\n",
      "Accuracy:  0.7208726945569051\n",
      "Auroc:  0.7218536757605506\n",
      "Auroc:  0.7977056473823816\n",
      "Total Time Elapsed: 498.42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.798"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(remove_stopwords(re.sub('\\d', '1', x))))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stem_text(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return \"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -144.56s\n",
      "(40013, 107611) (4446, 107611)\n",
      "X_train:  (40013, 107623)\n",
      "X_val:  (4446, 107623)\n",
      "CPU times: user 31min 34s, sys: 8.87 s, total: 31min 43s\n",
      "Wall time: 8min 34s\n",
      "Train\n",
      "Accuracy:  0.8048134356334191\n",
      "Auroc:  0.8068983045684058\n",
      "Auroc:  0.8923512706925427\n",
      "Validation\n",
      "Accuracy:  0.7233468286099866\n",
      "Auroc:  0.7248108152931327\n",
      "Auroc:  0.8028919328723256\n",
      "Total Time Elapsed: 681.42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.803"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Remove Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -138.49s\n",
      "(40013, 105020) (4446, 105020)\n",
      "X_train:  (40013, 105032)\n",
      "X_val:  (4446, 105032)\n",
      "CPU times: user 30min 34s, sys: 6.8 s, total: 30min 41s\n",
      "Wall time: 8min 17s\n",
      "Train\n",
      "Accuracy:  0.8046634843675805\n",
      "Auroc:  0.8067014075698676\n",
      "Auroc:  0.8918943174625651\n",
      "Validation\n",
      "Accuracy:  0.7197480881691408\n",
      "Auroc:  0.7212103541345116\n",
      "Auroc:  0.8015594042140087\n",
      "Total Time Elapsed: 657.98s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.802"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -133.52s\n",
      "(40013, 91252) (4446, 91252)\n",
      "X_train:  (40013, 91264)\n",
      "X_val:  (4446, 91264)\n",
      "CPU times: user 22min 6s, sys: 4.65 s, total: 22min 10s\n",
      "Wall time: 5min 55s\n",
      "Train\n",
      "Accuracy:  0.7973408642191288\n",
      "Auroc:  0.7999270296758744\n",
      "Auroc:  0.8843194966158119\n",
      "Validation\n",
      "Accuracy:  0.7199730094466936\n",
      "Auroc:  0.7234104873462248\n",
      "Auroc:  0.7969309453443083\n",
      "Total Time Elapsed: 502.54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.797"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(remove_stopwords(x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Stopwords Removal, Digits Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -111.76s\n",
      "(40013, 88806) (4446, 88806)\n",
      "X_train:  (40013, 88818)\n",
      "X_val:  (4446, 88818)\n",
      "CPU times: user 21min 43s, sys: 4.7 s, total: 21min 48s\n",
      "Wall time: 5min 49s\n",
      "Train\n",
      "Accuracy:  0.7973908479744083\n",
      "Auroc:  0.7996659429156177\n",
      "Auroc:  0.884612264522452\n",
      "Validation\n",
      "Accuracy:  0.7222222222222222\n",
      "Auroc:  0.7238732509275334\n",
      "Auroc:  0.7959654999359044\n",
      "Total Time Elapsed: 474.71s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.796"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(remove_stopwords(re.sub('\\d', '1', x))))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
