{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer, Preprocessing Variations\n",
    "## In this notebook, our objective is to find the optimal preprocessing steps that would maximize performance on both the train set and validation set using count vectorizer (Attempt to minimize both bias and variance). Often times, the best preprocessing might differ from dataset to dataset, and it may even be the case that if we use lesser preprocessing steps, we might obtain a better outcome. We use multiple preprocessing steps to see how well our model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import hyperopt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix, hstack, vstack\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Read in the preprocessed files\n",
    "train = pd.read_csv(\"../datasets/preprocessed_train.csv\")\n",
    "test = pd.read_csv(\"../datasets/preprocessed_test.csv\")\n",
    "train[\"Comment\"] = train[\"Comment\"].apply(lambda x: x.lower())\n",
    "test[\"Comment\"] = test[\"Comment\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get train, val and test data\n",
    "def get_train_test(train, test = None, ngram_range = (1,1), max_features=None, random_state=1, test_size=0.2, min_df=50):\n",
    "    \n",
    "    if type(test) != pd.core.frame.DataFrame:\n",
    "        # To check if we want to split into train val, or train test\n",
    "        \n",
    "        # Use only the train data for train val split\n",
    "        X = train.Processed\n",
    "        y = train.Outcome\n",
    "        \n",
    "        # split into train and test set, using random_state so that it is reproducable\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=random_state, test_size=test_size)\n",
    "        \n",
    "        # We use count vect character level analyser\n",
    "        count_vect_ngram_chars = CountVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df)\n",
    "\n",
    "        print(\"Fitting...\")\n",
    "        start = time.time()\n",
    "        # Fit transform only on the train set, use it to transform the val set\n",
    "        X_train_dtm =  count_vect_ngram_chars.fit_transform(X_train) \n",
    "        X_val_dtm =  count_vect_ngram_chars.transform(X_val) \n",
    "        print(f\"Operation Took {round(start-time.time(), 2)}s\")\n",
    "        print(X_train_dtm.shape, X_val_dtm.shape)\n",
    "\n",
    "        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n",
    "        add_var_df = train.loc[X_train.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            # Stacks horizontally, effectively increasing columns of features to include our EDA\n",
    "            X_train_dtm = hstack((X_train_dtm, var_sparse))\n",
    "\n",
    "        # Repeat the same for the validation set\n",
    "        add_var_df = train.loc[X_val.index].reset_index()[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            X_val_dtm = hstack((X_val_dtm, var_sparse))\n",
    "        \n",
    "        print(\"X_train: \", X_train_dtm.shape)\n",
    "        print(\"X_val: \", X_val_dtm.shape)\n",
    "        \n",
    "        return X_train_dtm, X_val_dtm, y_train, y_val\n",
    "    else:\n",
    "        # We use count vect character level analyser\n",
    "        count_vect_ngram_chars = CountVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=ngram_range, max_features=max_features, min_df=min_df)\n",
    "\n",
    "        print(\"Fitting...\")\n",
    "        start = time.time()\n",
    "        # Fit on train, transform train and test\n",
    "        X_train_dtm =  count_vect_ngram_chars.fit_transform(train.Processed) \n",
    "        X_test_dtm =  count_vect_ngram_chars.transform(test.Processed) \n",
    "        print(f\"Operation Took {time.time()-start}s\")\n",
    "        print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "\n",
    "        # Next, we need to add in the other variables from EDA, need to use scipy to maintain the sparse matrix or we will run out of memory\n",
    "        add_var_df = train[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            # Stacks horizontally, effectively increasing columns of features to include our EDA\n",
    "            X_train_dtm = hstack((X_train_dtm, var_sparse))\n",
    "\n",
    "        # Repeat the same for the test set\n",
    "        add_var_df = test[['num_numbers', 'prop_numbers', 'num_words',\n",
    "               'num_punctuation', 'prop_punctuation', 'nchar', 'word_density', 'noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']]\n",
    "        for column in add_var_df.columns:\n",
    "            var_sparse = add_var_df[column].values[:, None]\n",
    "            X_test_dtm = hstack((X_test_dtm, var_sparse))\n",
    "        \n",
    "        print(X_train_dtm.shape, X_test_dtm.shape)\n",
    "        \n",
    "        print(\"X_train: \", X_train_dtm.shape)\n",
    "        print(\"X_test: \", X_test_dtm.shape)\n",
    "        \n",
    "        return X_train_dtm, X_test_dtm, train.Outcome\n",
    "    \n",
    "def train_data(min_df = 50):\n",
    "    start = time.time()\n",
    "    # Using ngram_range of 2,5 because it was found to produce one of the best results for the model\n",
    "    X_train, X_val, y_train, y_val = get_train_test(train, test = None, ngram_range = (2,5), \n",
    "                        max_features=None, random_state=1, test_size=0.1, min_df = min_df)\n",
    "    \n",
    "    LG = LGBMClassifier(class_weight='balanced')\n",
    "    %time LG.fit(X_train, y_train)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    print(\"Train\")\n",
    "    y_pred_class = LG.predict(X_train)\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_train, y_pred_class))\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n",
    "    y_pred_class = LG.predict_proba(X_train)[:, 1]\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_train, y_pred_class))\n",
    "\n",
    "    y_pred_class = LG.predict(X_val)\n",
    "    print(\"Validation\")\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_val, y_pred_class))\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\n",
    "    y_pred_class = LG.predict_proba(X_val)[:, 1]\n",
    "    print(\"Auroc: \", metrics.roc_auc_score(y_val, y_pred_class))\n",
    "    print(f\"Total Time Elapsed: {round(time.time() - start, 2)}s\")\n",
    "    \n",
    "    return round(metrics.roc_auc_score(y_val, y_pred_class), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignore scipy warnings as it was intended to convert to sparse matrix below\n",
    "warnings.filterwarnings(\"ignore\", message=\"Converting data to scipy sparse matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -149.19s\n",
      "(40013, 96827) (4446, 96827)\n",
      "X_train:  (40013, 96839)\n",
      "X_val:  (4446, 96839)\n",
      "CPU times: user 15min 39s, sys: 3.86 s, total: 15min 43s\n",
      "Wall time: 4min 32s\n",
      "Train\n",
      "Accuracy:  0.7958913353160223\n",
      "Auroc:  0.7990208217230683\n",
      "Auroc:  0.8849861586527137\n",
      "Validation\n",
      "Accuracy:  0.7345928924876294\n",
      "Auroc:  0.7365992494135207\n",
      "Auroc:  0.8148530031175327\n",
      "Total Time Elapsed: 443.18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.815"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"]\n",
    "test[\"Processed\"] = test[\"Comment\"]\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -143.35s\n",
      "(40013, 108120) (4446, 108120)\n",
      "X_train:  (40013, 108132)\n",
      "X_val:  (4446, 108132)\n",
      "CPU times: user 14min 53s, sys: 2.27 s, total: 14min 56s\n",
      "Wall time: 4min 15s\n",
      "Train\n",
      "Accuracy:  0.7904431059905531\n",
      "Auroc:  0.7934347247792324\n",
      "Auroc:  0.8798952104995476\n",
      "Validation\n",
      "Accuracy:  0.7181736392262708\n",
      "Auroc:  0.7211688926575737\n",
      "Auroc:  0.8038123159311044\n",
      "Total Time Elapsed: 420.91s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.804"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: \"\".join(x.split()))\n",
    "test[\"Processed\"] = test[\"Comment\"].apply(lambda x: \"\".join(x.split()))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -142.97s\n",
      "(40013, 91675) (4446, 91675)\n",
      "X_train:  (40013, 91687)\n",
      "X_val:  (4446, 91687)\n",
      "CPU times: user 14min 45s, sys: 2.25 s, total: 14min 47s\n",
      "Wall time: 4min 13s\n",
      "Train\n",
      "Accuracy:  0.7967660510334141\n",
      "Auroc:  0.799772657220535\n",
      "Auroc:  0.8852668762254632\n",
      "Validation\n",
      "Accuracy:  0.7341430499325237\n",
      "Auroc:  0.7356710473169075\n",
      "Auroc:  0.8116025467840811\n",
      "Total Time Elapsed: 416.98s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.812"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: re.sub('\\d', '1', x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -124.57s\n",
      "(40013, 90352) (4446, 90352)\n",
      "X_train:  (40013, 90364)\n",
      "X_val:  (4446, 90364)\n",
      "CPU times: user 12min 33s, sys: 1.83 s, total: 12min 34s\n",
      "Wall time: 3min 30s\n",
      "Train\n",
      "Accuracy:  0.7918676430160198\n",
      "Auroc:  0.7951906824275033\n",
      "Auroc:  0.8791654882214287\n",
      "Validation\n",
      "Accuracy:  0.7278452541610436\n",
      "Auroc:  0.730915014672013\n",
      "Auroc:  0.8101237198125982\n",
      "Total Time Elapsed: 350.52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: remove_stopwords(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "# Helper function to lematize \n",
    "def lemm_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemma_word1 = [] \n",
    "    for token in doc:\n",
    "        lemma_word1.append(token.lemma_)\n",
    "    return \"\".join(lemma_word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -145.81s\n",
      "(40013, 104201) (4446, 104201)\n",
      "X_train:  (40013, 104213)\n",
      "X_val:  (4446, 104213)\n",
      "CPU times: user 14min 13s, sys: 2.9 s, total: 14min 16s\n",
      "Wall time: 4min 3s\n",
      "Train\n",
      "Accuracy:  0.7893934471296828\n",
      "Auroc:  0.792565618402093\n",
      "Auroc:  0.8775167360291246\n",
      "Validation\n",
      "Accuracy:  0.7199730094466936\n",
      "Auroc:  0.7218804251005108\n",
      "Auroc:  0.8009717417915021\n",
      "Total Time Elapsed: 411.45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.801"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits, Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -112.98s\n",
      "(40013, 85433) (4446, 85433)\n",
      "X_train:  (40013, 85445)\n",
      "X_val:  (4446, 85445)\n",
      "CPU times: user 11min 53s, sys: 1.69 s, total: 11min 54s\n",
      "Wall time: 3min 18s\n",
      "Train\n",
      "Accuracy:  0.7909929272986279\n",
      "Auroc:  0.7941460726777758\n",
      "Auroc:  0.8795049802031099\n",
      "Validation\n",
      "Accuracy:  0.7307692307692307\n",
      "Auroc:  0.7333879911513183\n",
      "Auroc:  0.8108805203693137\n",
      "Total Time Elapsed: 326.75s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.811"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: remove_stopwords(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -125.98s\n",
      "(40013, 90770) (4446, 90770)\n",
      "X_train:  (40013, 90782)\n",
      "X_val:  (4446, 90782)\n",
      "CPU times: user 10min 45s, sys: 1.86 s, total: 10min 47s\n",
      "Wall time: 3min 1s\n",
      "Train\n",
      "Accuracy:  0.7839702096818534\n",
      "Auroc:  0.7871799402763369\n",
      "Auroc:  0.8730315002251199\n",
      "Validation\n",
      "Accuracy:  0.7134502923976608\n",
      "Auroc:  0.7163366243937931\n",
      "Auroc:  0.7970118106566488\n",
      "Total Time Elapsed: 321.58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.797"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(remove_stopwords(x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Digits, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -135.86s\n",
      "(40013, 101656) (4446, 101656)\n",
      "X_train:  (40013, 101668)\n",
      "X_val:  (4446, 101668)\n",
      "CPU times: user 14min 12s, sys: 3.24 s, total: 14min 15s\n",
      "Wall time: 4min 2s\n",
      "Train\n",
      "Accuracy:  0.7880688776147752\n",
      "Auroc:  0.7910003288909877\n",
      "Auroc:  0.8773255399363092\n",
      "Validation\n",
      "Accuracy:  0.7174988753936122\n",
      "Auroc:  0.7208064391011152\n",
      "Auroc:  0.8000359264212079\n",
      "Total Time Elapsed: 400.13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Digits, Stopwords, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -112.4s\n",
      "(40013, 88374) (4446, 88374)\n",
      "X_train:  (40013, 88386)\n",
      "X_val:  (4446, 88386)\n",
      "CPU times: user 10min 27s, sys: 1.93 s, total: 10min 29s\n",
      "Wall time: 2min 55s\n",
      "Train\n",
      "Accuracy:  0.7838952340489341\n",
      "Auroc:  0.7868396347860698\n",
      "Auroc:  0.8731406982060164\n",
      "Validation\n",
      "Accuracy:  0.7170490328385065\n",
      "Auroc:  0.7200547826482382\n",
      "Auroc:  0.7992169850901236\n",
      "Total Time Elapsed: 301.85s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.799"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: lemm_text(remove_stopwords(re.sub('\\d', '1', x))))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stem_text(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return \"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -145.17s\n",
      "(40013, 107611) (4446, 107611)\n",
      "X_train:  (40013, 107623)\n",
      "X_val:  (4446, 107623)\n",
      "CPU times: user 13min 59s, sys: 2.23 s, total: 14min 1s\n",
      "Wall time: 3min 58s\n",
      "Train\n",
      "Accuracy:  0.7912428460750256\n",
      "Auroc:  0.7944445326062695\n",
      "Auroc:  0.8798651576394603\n",
      "Validation\n",
      "Accuracy:  0.7183985605038237\n",
      "Auroc:  0.7217212665277484\n",
      "Auroc:  0.7993692505637424\n",
      "Total Time Elapsed: 404.75s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.799"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(x))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Remove Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -131.37s\n",
      "(40013, 105020) (4446, 105020)\n",
      "X_train:  (40013, 105032)\n",
      "X_val:  (4446, 105032)\n",
      "CPU times: user 14min, sys: 2.47 s, total: 14min 2s\n",
      "Wall time: 3min 58s\n",
      "Train\n",
      "Accuracy:  0.7905180816234724\n",
      "Auroc:  0.7937623009541839\n",
      "Auroc:  0.8791270451815256\n",
      "Validation\n",
      "Accuracy:  0.7195231668915879\n",
      "Auroc:  0.7223057396058752\n",
      "Auroc:  0.8002814030563796\n",
      "Total Time Elapsed: 390.66s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(re.sub('\\d', '1', x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -118.75s\n",
      "(40013, 91252) (4446, 91252)\n",
      "X_train:  (40013, 91264)\n",
      "X_val:  (4446, 91264)\n",
      "CPU times: user 9min 51s, sys: 1.64 s, total: 9min 53s\n",
      "Wall time: 2min 46s\n",
      "Train\n",
      "Accuracy:  0.7821458026141505\n",
      "Auroc:  0.7856096440704238\n",
      "Auroc:  0.8722938281532686\n",
      "Validation\n",
      "Accuracy:  0.7165991902834008\n",
      "Auroc:  0.7194796718390973\n",
      "Auroc:  0.7961928693255648\n",
      "Total Time Elapsed: 297.63s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.796"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(remove_stopwords(x)))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, Stopwords Removal, Digits Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting...\n",
      "Operation Took -107.88s\n",
      "(40013, 88806) (4446, 88806)\n",
      "X_train:  (40013, 88818)\n",
      "X_val:  (4446, 88818)\n",
      "CPU times: user 9min 50s, sys: 2.71 s, total: 9min 52s\n",
      "Wall time: 2min 46s\n",
      "Train\n",
      "Accuracy:  0.7825956564116662\n",
      "Auroc:  0.7858120909489095\n",
      "Auroc:  0.8725080207758469\n",
      "Validation\n",
      "Accuracy:  0.7159244264507423\n",
      "Auroc:  0.7185875813514302\n",
      "Auroc:  0.795761381895594\n",
      "Total Time Elapsed: 293.61s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.796"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Processed\"] = train[\"Comment\"].apply(lambda x: stem_text(remove_stopwords(re.sub('\\d', '1', x))))\n",
    "train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
